---
title: Aria
date: 2024-10-10
external_link: https://github.com/rhymes-ai/Aria
tags:
  - Native Multimodal MoE
  - NaViT
  - Pretrain & SFT
---

Aria is a multimodal native MoE model. It features：1️⃣State-of-the-art performance on various multimodal and language tasks, superior in video and document understanding; 2️⃣Long multimodal context window of 64K tokens; 3️⃣3.9B activated parameters per token, enabling fast inference speed and low fine-tuning cost.

<!--more-->
