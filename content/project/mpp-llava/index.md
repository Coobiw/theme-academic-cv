---
title: MPP-Qwen-Next
date: 2023-12-25
external_link: https://github.com/Coobiw/MPP-LLaVA
tags:
  - MLLM Training Framework (Supporting Multimodal Pipeline Parallel)
  - Pretrain & SFT
---

The Repo supports {video/image/multi-image} {single/multi-turn} conversations. All 7B/14B llava-like training is conducted on 3090/4090 GPUs. ***To prevent poverty (24GB of VRAM) from limiting imagination***, I implemented an MLLM version based on deepspeed Pipeline Parallel.
<!--more-->
